{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775RPVD5fi02"
      },
      "outputs": [],
      "source": [
        "!pip install -U dataests transformers==4.38.2\n",
        "!pip install torch accelerate datasets wandb\n",
        "!git clone https://github.com/aloobun/bitnet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bitnet\n",
        "!huggingface-cli login --token \"<token>\""
      ],
      "metadata": {
        "id": "xSOG-u6RxNf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_name = \"JeanKaddour/minipile\"\n",
        "ds_train = load_dataset(ds_name, split=\"train\")\n",
        "ds_valid = load_dataset(ds_name, split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train.shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid.shuffle().select(range(500)),\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "Zh8GIJvVxNid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
      ],
      "metadata": {
        "id": "f87k9XruxNk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "4IXGprVixNnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from modeling_bitllama import BitLlamaConfig, BitLlamaForCausalLM\n",
        "BitLlamaConfig.register_for_auto_class()\n",
        "BitLlamaForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")"
      ],
      "metadata": {
        "id": "3CtLf0RSxNpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from bitnet import BitLinear\n",
        "\n",
        "activation_layers = [nn.SiLU, nn.ReLU, nn.GELU]\n",
        "\n",
        "def replace_linears_in_hf(model, parent=None):\n",
        "    \"\"\"\n",
        "    Replaces all instances of nn.Linear in the given model with BitLinear.\n",
        "    If a Linear layer is immediately followed by a specified activation layer, sets flg_before_linear to False.\n",
        "    refers: https://github.com/kyegomez/BitNet/blob/d32fb9b8d83028d9571bfb213d8c5e4e7b915e42/bitnet/replace_hf.py#L6\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model to modify.\n",
        "        parent (nn.Module): The parent module of the current module being processed.\n",
        "    \"\"\"\n",
        "    children = list(model.named_children())\n",
        "    for i, (name, module) in enumerate(children):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Check if the next module is in the specified activation layers\n",
        "            next_module_is_activation = (\n",
        "                i + 1 < len(children) and any(isinstance(children[i + 1][1], layer) for layer in activation_layers)\n",
        "            )\n",
        "            # Replace the nn.Linear with BitLinear\n",
        "            setattr(\n",
        "                model,\n",
        "                name,\n",
        "                BitLinear(\n",
        "                    in_features=module.in_features,\n",
        "                    out_features=module.out_features,\n",
        "                    bias=module.bias is not None,\n",
        "                    flg_before_linear=not next_module_is_activation,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            # Recursively apply to child modules\n",
        "            replace_linears_in_hf(module, parent=model)"
      ],
      "metadata": {
        "id": "M3VGvRd8xNsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BitLlamaConfig(\n",
        "    model_type=\"bit_llama\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    hidden_size=768,\n",
        "    max_position_embeddings=512,\n",
        "    intermediate_size=1536,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=12,\n",
        "    num_key_value_heads=4,\n",
        "    torch_dtype=torch.float32,\n",
        "    rms_norm_eps=1e-05,\n",
        ")\n",
        "print(config)\n",
        "\n",
        "\n",
        "model = BitLlamaForCausalLM(config)\n",
        "print(model)\n",
        "\n",
        "\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"model size: {model_size/1000**2:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "YKEn8pGYxNus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bitLlama-110m\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    logging_steps=1000,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=500,\n",
        "    lr_scheduler_type=\"polynomial\",\n",
        "    learning_rate=2.4e-3,\n",
        "    save_steps=2000,\n",
        "    bf16=False,\n",
        "    push_to_hub=True,\n",
        "    report_to=\"wandb\",\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ],
      "metadata": {
        "id": "N_XviG1bxNxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "QGXms16cxNz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "xeONhDYWxN2y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}